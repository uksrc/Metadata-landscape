\section{Metadata Usecases}\label{sec:reqs}

This section describes use cases for metadata within the SKA.
The sources for these metadata include a "literature review of available SKA documents" and a survey/interview with the leaders for each of the demonstrator cases. 

\subsection{SRCNet Use Cases}

The SRCNet has already defined a number of use cases (UCs) which are described as "tasks an astronomer as well as SKAO/SRC Staff would be likely to perform within the
SRCNet" \footnote{tasks an astronomer as well as SKAO/SRC Staff would be likely to perform within the SRCNet}. \textcolor{cyan}{Is there a reference we need to add in here?}
These UCs span a wide variety of topics, not all of which concern the use of metadata.
The following UCs are all of those contained within the UC document which will have implications for the metadata landscape of the SKA. 
\\
\begin{itemize}[label={}]
    \item {\bf UC\_SRC\_1} - To browse the entire SKA data collection so that I can find data products which are suitable for my science goals.
    \begin{itemize}[label={}]
        \item Data Product metadata; including footprint/IVOA/ALADIN-cross-compatible, new Radio-observation identifiers like array format/baselines/resolution/beam etc. (see ObsCoreExtensionForRadio draft document).  Validated ADP from other users would be possible in this use case, so SRCNet ADP provenance scripts and an ADP version of QA/vetting would also need to provide metadata.
    \end{itemize}
    \item {\bf UC\_SRC\_2} - To inspect (on the fly) and retrieve data products available at a certain coordinate or for a long list of positions.
    \begin{itemize}[label={}]
        \item See UC\_SRC\_1, with option needed for database input/output of list in a file.  
        Do we need user input/output option metadata identifier to keep track of user interface preferences, or is this folded into GUI code, with no additional db/metadata need here?  Is it useful to track that granularity of user interaction?
    \end{itemize}
    \item {\bf UC\_SRC\_3} - To make my data products and associated workflows public to maximise scientific return and avoid duplication of effort.
    \begin{itemize}[label={}]
        \item Provenance/metadata-creating tools/scripts that run automatically in the background alongside of scientists/user data analysis, and database table systems to hold the metadata… i.e. All processing to be done on SRCNet machines so that scripts will automatically collect this provenance metadata and ingest it without user having to initiate an extra task.  
        Metadata would need to be carried over from Project and Data Product on whether ADP can/cannot be public yet, either by mandatory proprietary period or, beyond that when original observational data products become public, by PI or designated user decision.
    \end{itemize}
    \item {\bf UC\_SRC\_4} - To browse the QA information associated with ODPs.
    \begin{itemize}[label={}]
        \item This should/would be tied into any ObsID from the ODA (Observations Data Archive) on SKAO side, and thus ingestible to SRCNet… or affiliated with whatever unit of observation/DP the QA is relevant to (Execution Block?).  If Products don’t pass QA, which information is needed in SRCNet by users?  Obs, Point of failure, what process failed, text error messages?   "My observation didn’t pass QA but I need to be able to see/know that"…what do scientists/users need to know? 
    \end{itemize}
    \item {\bf UC\_SRC\_5} - To track the workflow history of a data product so that I can understand how it was generated by the Observatory and/or users.
    \begin{itemize}[label={}]
        \item Provenance and tracking scripts running alongside everything, to generate and ingest metadata, both on ODA/SKAO side for initial pipelines/cal, and also on SRCNet for ADP or any and all user calibration/reduction/processing pipeline branches that stem from original ingest.   Will this function best as a pre-SRCNet and post-SRCNet separate workflow histories, or does it need to blend for a seamless schematic/picture of Lifecycle of Data Product for the user from observation to a given ADP? 
    \end{itemize}
    \item {\bf UC\_SRC\_6} - To optionally transfer a dataset that fails QA to the SRCNet so that I can investigate the cause of the failure.
    \begin{itemize}[label={}]
        \item See UC\_SRC\_4.  Data products wouldn’t get ingested across to SRCNet itself, but a subset/view of relevant metadata regarding the failed data would need to be ingested by SRCNet DA.  Additionally, assessment of resources would need to be made due to potentially lower priority of a failed QA data product coming across and sucking up space/processing, so resources metadata and resources analysis scripts would be relevant here.
    \end{itemize}
    \item {\bf UC\_SRC\_7} - To Inspect metadata, provenance, and available workflows associated with user-generated ADPs to assess their robustness.
    \begin{itemize}[label={}]
        \item This sounds like a form of QA or verification/vetting for ADPs.  The same metadata is needed for this as any finished ADP data path.  If a process like QA will be developed within the SRCNet/SKAO so that, by way of the data discovery tools, a user may be able to decide whether a user-created ADP is useful, reliable, and valid for them, we need to know if there exists a standard or written outline for such assessment.  Do SKAO have documentation illustrating what ADP QA/validation should look like in the SRCNet?  One might imagine a scenario where, if an ADP is validated by several end users, that could count like up-votes.  Thus the metadata DB would need identifiers for counting these upvotes.  Otherwise, users could view the provenance/rerun the pipelines themselves for a designated time period, give the ADP an up-vote, and eventually the more upvotes an ADP has, the less likely it would be that the next user will need to go through the reprocessing/resource usage to re-do ADP user-based pipeline in SRCNet.  This could aid in analysis of resource-worthiness.
    \end{itemize}
    \item {\bf UC\_SRC\_8} - To monitor usage at the project level to ensure it is in line with appropriate policies, and take appropriate action if not.
    \begin{itemize}[label={}]
        \item Does a clear definition of ‘Project’ from SKAO to define/understand “usage at the product level exist in a document already?  Also, which policies are we talking about, and which actions?  Is one example whether proprietary data are kept proprietary?  A concrete example case may be useful so that the UC can be understood.  Also, will Rucio be utilised in any way pre-SRCNet on the SKAO side for initial data reduction pipeline/flagging/calibration?  If not, then we will need usage/resource metadata pulled through on ingest from ODA to UKSRC, and transposed/translated/sync’d with metadata nomenclature for Rucio/SRCNet.  
    \end{itemize}
    \item {\bf UC\_SRC\_9} - To track my processing time and data storage so that I can manage my compute resources.
    \begin{itemize}[label={}]
        \item To estimate resources that will be needed by required processes and/or requested user processes, the system will need to accumulate metadata on machine status and resource usage during any data processing and “learn”, so predictions can be made by the system.  Also, the same metadata will be needed to check what/where/who in SRCNet hardware has enough available resources. Systems monitoring metadata will need to be ingested from the various storage/processors, and intersect with calculations/scripts that will also see averages/previous data from processes for comparing need to availability.  Does metadata for the systems availability come from Rucio?  If so, do we understand the identifiers, and can we use the same as Rucio already uses, for easiest sync?  What units are we using (which granularity)?
    \end{itemize}
    \item {\bf UC\_SRC\_10} - To estimate the data size and processing time needed for my project before submitting a processing proposal.
    \begin{itemize}[label={}]
        \item SKAO Tools should handle this in proposals for obs, as they are connected, and if SRCNet predicts inability to process a project at the estimated ingest time, then SKAO Time allocation procedure would need to know!!   This might include projects that are outside the scope of available tech/hardware resources, allowing for proposer ignorance.  SRCNet (as the “face” of SKA) needs to be connected with Observing proposal tools so that the Time Allocation process can assess resource usage as a facet of Project/Proposal ranking.
    \end{itemize}
    \item {\bf UC\_SRC\_11} - To monitor total SRCNet account usage per user and per user group, aggregated across all SRCNet resources, currently and historically, so that I can check if a user, user group or project is still within its allocated usage and take appropriate action as required.
    \begin{itemize}[label={}]
        \item User privacy could be an issue with this UC, if tracking geographical regions/SRC or country affiliations.  How often is calculation to be done for this on a per-user basis?  Do we need IVOA for this?  Metadata from Rucio on usage?
    \end{itemize}
    \item {\bf UC\_SRC\_12} - To monitor pledged and used resources per SRC site, allocated SRC project and nation, to track usage (e.g. quarterly, yearly and all historical data), so that they may be used for future plans accordingly and shared with colleagues outside the SRCNet ecosystem.
    \begin{itemize}[label={}]
        \item See above UC\_SRC\_11… and additionally, if SRC usage is spread across whole Net (as required by SKAO SRCNet plan, and one person from anywhere uses resources across all, this illustrates the importance that all SRCNet us the same DB hierachies.  If authenticity/security is solid for logins, and person metadata is ingested, this should be combinable in a DB Table/View with Rucio usage metadata, if it exists.    
        This UC potentially crosses over into Bibliometrics territory, in terms of SRC project/nation usage distribution reporting.
    \end{itemize}
    \item {\bf UC\_SRC\_13} - To monitor the progress of my processing jobs (e.g. status, used resources) so that I can manage the data reduction and identify any issues with the processing.
    \begin{itemize}[label={}]
        \item See UC\_SRC\_9 for resources usage.  As far as status/progress, what is already built in to data analysis tools via Canfar/Jupyter and can we use the metadata already provided by these tools, or do we need to define it?  Or is this more on a jobs/systems-level and needing to pull metadata from the Rucio machine monitoring systems that are likely in place?
    \end{itemize}
    \item {\bf UC\_SRC\_14} - To Monitor and understand the popularity of data products or data collections so that I can ensure that appropriate overall QoS is being provided to popular (or unpopular) data products/collections.
    \begin{itemize}[label={}]
        \item Bibliography database needed here; see arXiv preprint from 2024: 
        https://arxiv.org/pdf/2401.00060 
        Gives recommendations for identifiers and info needed in Biblio archive to effectively track sci usage.  Web scraping tools can be used to collect paper references/Bibcodes, then humans are needed to parse/input relevant metadata.  There is scope for AI to do some parsing and DB input, with periodic human spot-checks.
    \end{itemize}
\end{itemize}


Add a short description on the specifics on the metadata needed, group by the required information and include all UCs which require it. 
Collate this information in a matrix later on. 
\\
\textcolor{cyan}{
Is this display to be like the the table in 3.1?  
What level of detail are we going for in terms of metatdata and can we reference to previously made DMs ??
Also, "short" and "specifics" seem to be at odds here.  Which is more important?  
}

\subsection{Demonstrator Cases of the SKA - survey}

This section will be populated with the use cases that result from the responses of the demonstrator case survey. The current use cases within this section are from discussions with the demonstrator case scientists.

\subsubsection{From Discussions}
\begin{itemize}[label={}]
    \item {\bf UC 1} - Storage vs Re-computation. The determination of whether a piece of data should be stored or re-computed/re-downloaded. Requires information on the size of the data, time taken to compute/download, number of times it has been accessed, available resources (compute and storage).
    \item {\bf UC 2} - Prediction of Pipeline Performance. The prediction of required resources (e.g. compute, memory, time, quality, etc.) to execute a pipeline, based upon the pipeline components and input data scale. Requires identifiers for the different pipeline components (fucntions, modules, etc.), metadata on memory consumption and time for individual components, versioning information, data source metadata for scaling, quality metrics. 
    \item {\bf UC 3} - Recommendation of Components. Requires description on the purpose of components, data flow to recommend next steps based upon previous, parameter key/values if recommendation on that level is required, data source metadata to recommend steps with appropriate scaling etc., quality metrics.  
    \item {\bf UC 4} - Retrieve all Data Products from a given Source. Data flow, data source metadata, data product metadata, identifiers on all aforementioned metadata. 
    \item {\bf UC 5} - Anomaly detection. Determination of: the difference between two pipeline executions; the source of a fault within a pipeline execution; or unusual processing/parameter values within a pipeline execution.
    \item {\bf UC 6} - Re-computation. The ability to re-execute a pipeline from the provenance.
    \item {\bf UC 7} - Retrieve all data products from person/organisation.
\end{itemize}

\subsection{E-Merlin Usecases}\label{sec:reqs}

This section describes use cases for metadata within e-Merlin.

Names of ancillary or associated sources and their roles - ask Paul what these products are and what the roles are? Also maybe a use case around it

\subsection{As an astronomer}

{\bf Use case 1} - As an astronomer, I want to be able to browse the e-Merlin archive so that I can find data products that are suitable for my science goals.
\\
{\bf Requirements 1} - Observational metadata including: target (name, id, etc.), WCS information (RA/DEC), date/time of observation, observing  frequency, spectral line data, baseline, resolution, beam, field of view, PI or researchers, Telescopes present, number of visits, Polarisation
\\\\
{\bf Use Case 2} - As an astronomer, I want browse the quality assessment information associated with the retrieved data products.
\\
{\bf Requirements 2} - Quality from the raw data table (perhaps not populated), quality in the processing (\href{https://www.e-merlin.ac.uk/distribute/CY8/TS8004/TS8004_C_001_20190801/weblog/pipelineinfo.html}{this} page as a starting point for failed steps) Requested or predicted sensitivity, Telescopes scheduled, Comments on observing, other information used as indicators of quality e.g. UV plane
\\\\
{\bf Use Case 3} - As an astronomer, track the workflow history of a data product so that I can understand how it was generated by the Observatory.
\\
{\bf Requirements 3} - Pipeline version, input parameters, raw data identifier
\\\\
{\bf Use Case 4} - As an astronomer, I want to be able to find data that is closely linked to my dataset. For example, data that shares a project code, principle investigator, or was derived from the same set of raw data. 
\\
{\bf Requirements 4} - Project code, PI, raw data identifier
\\\\
{\bf Use Case 5} - As an astronomer, I want to be able to use geometric spatial searches for my target data (cone search etc.).
\\
{\bf Requirements 5} - PostgresSQL, WCS info, field of view
\\\\
{\bf Use Case 6} - As an astronomer I want to be able to search for ancillary sources to my target data and discover their roles. 
\\
{\bf Requirements 6} - Ancillary target names and roles
\\\\
\subsection{As an operator}
{\bf Use Case 7} - As an operator I want to be able to re-compute any data product based upon the metadata. 
\\
{\bf Requirements 7} - Pipeline used, versions, parameters, raw data, runtime environment?
\\\\
{\bf Use Case 8} - As an operator I want to be able to determine if any given data product should be stored or re-computed on demand. 
\\
{\bf Requirements 8} - available resources (storage and compute), memory consumption of pipeline, process timings, data (physical) size, number of backups necessary, number of times data is accessed, raw data location/identifiers
\\\\
{\bf Use Case 9} - As an operator I want to be able to detect anomalies within the data processing or data itself. 
\\
{\bf Requirements 9} - Quality metrics, parameters, processing time, memory consumption, identifiers, raw data links, 
\\\\
{\bf Use Case 10} - As an operator I want to be able to determine if the needs of a proposal can be met by existing data products
\\
{\bf Requirements 10} - Observational metadata (date/time, location, field of view, polarisation, frequency, quality, etc.)
\\\\
{\bf Use Case 11} - As an operator, I want to be able to selectively determine when and to whom data should available 
\\
{\bf Requirements 11} - Permissions something something, time/date, project code, user information, agreements info (if they can be different)
\\\\
{\bf Use Case 12} - As an operator, I want to be able to infer data quality from available metrics
\\
{\bf Requirements 12} - resolution, weather, time/date, elevation, pipeline version/name/provenance
\\\\
\subsection{Non-Human Use Cases}
{\bf Use Case 13} - As a computer, I want to be able to process observational data without human interaction. 
\\
% {\bf Use Case 13 (operator perspective)} - As an operator, I want to automate the ingestion of observed data into the e-Merlin pipeline
\\
{\bf Requirements} - Raw data location/identifiers, pipeline info (location, version, available deployments),
\\\\
{\bf Use Case 14} -  As a computer, I want to be able to reprocess data products in the case of poor data quality or anomalous results.   
\\
{\bf Requirements 14} - Raw data location/identifiers, pipeline info (location, version, available deployments), quality/success of data processing, information to create a baseline of expected results (possibly target name and identifiers/locations to associated data products themselves for comparison)
\\\\
{\bf Use Case 15} - As a computer, I want to prevent proprietary science data products and certain metadata from being accessed by unauthorised users, and allow public data to be authorised and publicly searchable by any users.
\\
{\bf Requirements 15} - Release date, ObsID/observation identifier/projectID, PI name, authentication credentials, public/proprietary flag
\\\\
{\bf Use Case 16} - As a computer, I want to notice new data products and ingest them into a database. 
\\
{\bf Requirements 16} - ObsID/run/date file path component query on database, code to check for new products, code to generate metadata to populate db tables.